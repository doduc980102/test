A Novel Framework using Apache Spark for Privacy Preservation of Healthcare Big Data  

Abstract — The privacy of sensitive data in healthcare big data should be protected so that it does not intrude the privacy of specific patient or transactions of health care organizations. Privacy protection is a major concern in big data, therefore, demanding resilient approaches for the safeguarding of customer privacy. The proposed novel framework spark employs K-anonymization and L-diversity to mask the personal sensitive information and Apache Spark to handle health care big data in a faster and effective way. Thus proposed approach make sure that shared data will not disclose the original data and segregation of sensitive data happens before it is a move to HDFS. 

Keywords — Big data, Privacy preservation, anonymization.  
I. INTRODUCTION 

Implausible volumes of data are being generated by different business organizations like the health care sector, finance sector, e-Commerce, retail and supply chain, etc. by advanced innovation of digital technology. Individuals, as well as machines, generate data, for example, IoT – streaming data, logs from websites, etc. The huge data streamed in a variety of types and sources with more velocity is known as Big data. These qualities of Big data normally connect with extra challenges in storage, processing and application of methods for mining insights. Big data has become a very dynamic research zone in recent years. The information streaming speed is increasing so quickly that it is becoming very hard to manage it by conventional techniques or systems. For now, big data could be structured, semi-structured, or unstructured, which includes more difficulties when performing information storage and managing jobs. Thus better approaches to store and manage information progressively, in reality, is needed. 

Big data, if captured and analyzed sensibly, can be transformed into useful knowledge which adds significant value to the organization. It can assist organizations and companies to improve the strategy building and can offer the best possible business Intelligence. Huge voluminous data is produced every minute by social networking and mobile applications. The enormous volumes of data produced from the different varied sources can be handled and processed to help better strategy building decision making. Even though data analytics is helpful in strategy building and decision making, it’s leading to severe privacy concerns. At present, privacy-preserving data mining methods can be roughly divided from four aspects  

II. PROBLEM DEFINITION 

The traditional security techniques such as encryption, key generation, anonymization, rule generation and privacy preservation were developed for ensuring the security and confidentiality of the data stored in a cloud system when the data is huge. In a distributed storage system, providing security to personal identity information is highly difficult. To provide security to the personal data, anonymization technique is developed. In this key generation and distribution plays an essential role which is one of the major security issues. To overcome this issue, the dynamic authentication protocol can be used. In a cloud system, when the data is accessed, the secured audit must be performed. The available security mechanisms for processing big data in a  secured way define the policies to restrict data access. 

Additional security measures are required during data collection, analysis, data transferring and storing. Thus in many application areas of big data, it is essential to provide privacy, security and confidentiality. Privacy protection is one of most apprehensive concerns in big data thus necessitating robust methods for reservation of customer privacy. 

Anonymization of data is an excellent way to safeguard data privacy[1], and multidimensional anonymization techniques are widely embraced by the current anonymization schemes[2]. This applies to conceal the information that is vulnerable to privacy such as names or sensitive data from record owners such as clients and patients.

Furthermore, an individual’s privacy can be effectively protected while data consumers are subjected to certain aggregate information for extensive analysis and mining. This scheme’s scalability issues have drawn attention from privacy research groups, and several methods to tackle it has been proposed to incorporate spatial indexing or sampling technologies[3]. Nonetheless, current solutions suffer from severe scalability or IT cost issues, when managing big data. 

III. LITERATURE REVIEW 

Srinivas et al [4] have described the evolution of apache-spark, features of apache spark – its ease of use, the combination of SQL, streaming and complex analytics, provides built-in API in Java, Scala or python. The components, its operations and issues are discussed. 

Rabi Prasad Padhy [5] has given an overview of big data, architecture and components of Hadoop, Hadoop Cluster File System (HCFS), Map Reduce programming model, its applications and implementations in a cloud environment is described in detail. 

DevarajDass et al[6] have briefed on how to add security to Hadoop. It is difficult because the file system is partitioned and distributed. The client access the data from the data nodes which are not in the ACL(Access Control List). So, authorizing data becomes impossible. The workflow of the system can be accessed by the users through auxiliary services which lead to various issues. The various applications accessing HDFS, the user submitting a job to MapReduce clusters and the  identity-related threats are discussed. In RPC (Remote Procedure Call), the authentication is insecure, because the user’s login name is sent across the OS. A new layer, called Simple Authentication and Security Layer (SASL) which negotiates a subprotocol to use Hadoop. 

Haoyuan et al [7] have evaluated on Tachyon- a distributed file system, it outperforms in memory by 110 times faster in writing. End-to-end latency is improved 4 times better. With the evaluation on edge checkpointing algorithm, iterative workflow with 100 jobs are simulated and the results are compared with various fixed checkpoint intervals. The average recovery time is calculated. 

Qifan Pu et al [8] has studied the fair allocation problem in memory for multiple users and in terms of sharing files. Fair Ride is allocated and evaluated on Tachyon, which can be used as a caching system and it supports in-memory sharing across different cluster computation frameworks or applications. 

Durdabal et al [9] have suggested a framework on Hadoop. To address the various security threats – chosen plain text, identity compromisation, stolen verifier, an authentication protocol for fault tolerance which is suitable for Hadoop framework called HEAP are suggested. In HEAP, authentication is verified using the signature in digital and verification by utilizing both Advanced Encryption Standard (AES) and Elliptic Curve Cryptography (ECC). 

IV. COMPARATIVE ANALYSIS OF EXISTING ANONYMIZATION TECHNIQUES 

A. Existing Privacy Preservation Techniques 

An individual can decide which information can be shared or restrict the access to ensure his privacy which is a basic requirement. If the key information is publicized then it is very vulnerable as the data is at the control of information holder. Here information holders will be websites, mobile apps, social networking applications, e-commerce websites, banks, hospitals etc. Guaranteeing privacy is crucial in present computing atmosphere. It is necessary to recognize the need for sensitive data preservation and thereby suitable protection techniques to develop and implement the privacy preserved system. The information holders must make sure data privacy of the user’s data. Various Privacy protective methods are majorly based on Anonymization. 

A range of privacy preservation methods is as below. 

    I. Anonymization Technique 
        • K-anonymity 
        • L-diversity 
        • T-closeness 
    II. Randomization Technique 
    III. Cryptographic Technique 
    IV. Data distribution Technique 

I. Anonymization 

Data gathering for analytics causes massive privacy issues. Person identifiable information (PII) is very tough because the information is shared too quickly. Users information ought to be anonymized (de-identified) and then be released to data analyser via protected channels. Anonymization is that the irreversible elimination of knowledge that would guide to a private being recognized, either on the premise of the removed info or together with different information. Anonymization operations used to preserve data privacy are listed below. 

a. Generalization: It is replacing the specific Quasi-identifier (QID) values with a reduced specific description with added generalization. Here the specific data with general data in the categorization of a data is swapped. For example, while generalizing a designation, swapDeveloper or Testerdata values with Employee is added. 

b. Suppression: Data values are swapped with some special characters like “*” in suppression operation. This way disclosing of real data can be avoided. Suppression includes suppression of records, suppression of values, and cell suppression of cell. 

c. Anatomization: In anatomization, the relationship between the quasi-identifiers and sensitive attributes is dissociated. The data on Quasi-identifiers and sensitive attributes are put in in two different tables. While the first table contains the quasi-identifiers and the other table lists sensitive attributes with a common attribute which is referred to as Groupid. The Groupid will have identical group value for the same group related to the confidential values. 

d. Permutation: Permutation is disassociating the QID and SA through partition datasets into groups  and shuffling their sensitive values within each group. 

K-anonymity: It is a fascinating technique for big data privacy preservation based on anonymization. Here the quasi identifier attributes are dealt with. It is a unique approach to implementing kanonymization for QID data. It’s a novel technique named “k-anonymity without the prior value of the threshold k”. But, in other many kanonymity algorithms, the k-anonymity’s threshold-k is well-known before the data anonymization. 

L-diversity is proposed to defeat the limitations of k-anonymity. This novel technique is introduced to ensure privacy preservation by avoiding data attribute leak even with background knowledge. This operation “well-represent” the sensitive attributes in each group. Sensitive data attributes are varied among every quasi-identifier equivalence class. This is a K-anonymity’s modification operation. But, L diversity suffers from homogeneity attack. 

T-closeness: Refinement of l-diversity by decreasing the granularity of the interpreted data is t-closeness operation. The analyzer’s scope of knowledge on a specific data is partial while the facts are not limited to the overall table containing the datasets. Therefore, this lessens the association among the QID attributes and sensitive attributes. 

II. Randomization technique: 

The modification process of data values by adding noise using probability distribution is known as Randomization. It is widely implemented in surveys, sentiment analysis etc. It doesn’t other records knowledge and can be implemented during data compilation and pre-processing time. Randomization technique does not apply to large datasets because of time complication and data usefulness. 

III. Data distribution technique 

Data distribution technique is distributing the data across many sites. It is in two ways: 

a. Horizontal distribution: In Horizontal distribution information is scattered among numerous sites with the equal valued identifiers. This technique can be allied on the information without really sharing the data. Here, the data values are scattered at diverse sites of various organizations.  

b. Vertical distribution: Vertical distribution distributes the user’s specific data in diverse nodes under control of various organizations. Here, every site stores a partial set of identifiers of an individual. Data values will be pooled from all these sites for data analytics and then, there is a possibility of privacy breach vulnerability. Ensuring privacy during data analytics of vertically distributed data is difficult as the data values are scattered among various sites under the control of various organizations. 

IV. Cryptographic techniques This technique ensures privacy by encrypting sensitive. It provides a proper toolset for algorithms of cryptography. Maximal privacy is achieved which hides all vital data except for the required output of the function. This technique attempts to model the world which is both realistic and common. The data analyser may encode the data before discharging the equivalent for analysis. The crucial problem is encoding vast scale data using conventional encryption method is very difficult. So, it does not hold good for large databases. Also, this approach is hard to scale when more parties are involved. 

B. Map Reduce : 
Map Reduce is a processing layer in Hadoop. It is used to process large dataset stored in Hadoop HDFS (Hadoop distributed file system). In this, Data are processed in parallel on a cluster of machines. The framework of map Reduce consists of two stages, namely Map and Reduce. A set of data is collected in Map as input and they are converted into another set of data in which every data is broken into pair based on a key value. The output from this is taken as an input to reducers and finally, data are combined to produce the result.  

C. Spark: 
Apache Spark is an open-source processing engine with speed, ease of use, and capabilities to do analytics. It is a general-purpose cluster computing system with sophisticated Java, Scala, Python and R APIs. It is designed for batch applications and real-time streaming. It is used to run an application 100 times faster (than mapreduce) with the support of in-memory. It also supports advanced analytics like Machine Learning, streaming the data, algorithms on Graph and SQL queries. Structured data can be processed in spark SQL. The streaming of data is handled by spark streaming in which they are converted into micro-batches. Spark MLib is used to implement machine learning with the support of many predefined algorithms like iterative. Spark Graph X enables graph data processing. Table 1 illustrates a comparison between Map reduce and Spark.  

V. PROPOSED METHODOLOGY 

The privacy of sensitive data in healthcare big data should be protected so that it does not intrude the privacy of specific patient or transactions of health care organizations. The proposed novel framework spark employs K anonymization and L- diversity to mask the personal sensitive information and Apache Spark to handle health care big data in a faster and effective way. Even though K -anonymization mask personal identity, which is not enough for full privacy preservation, L -diversity is used to mask, common attributes from the records. The k-anonymity approach is deficient of diversity which leads to ldiversity [10]. Figure 2 illustrates the proposed Framework  

The proposed privacy preservation algorithms proposed in this paper services α-index table and β-index table for segregating the data into diverse sets and afterwards creating anonymized data store. α-index table preserves the personal or identifiable information like name, address etc and thereby membership details from the dataset and β- index gives health data or sensitive attributes (like income, bank details) of each member from the records. The in-memory operation (read and write operation will be applied to memory)is made possible through Apache Spark to handle bigger set of unorganised data. The HBase table comprises  the anonymized Data produced by k–Optimize algorithm. The approach envisions the dissemination of the data on HDFS. 

Algorithm 1: Privacy Preservation using K-Anonymization and Apache Spark 

Step 1: User login and queries the health care big data. 
Step 2: Availability of required data in an α-index table in dataset N is verified 
Step 3: Corresponding sensitive data in the β-index table is verified. 
Step 4: Member details and corresponding sensitive data is matched to fetch the identifier attributes, Quasi-Identifier attributes and sensitive personal information 
Step 5: Privacy rules are defined and anonymized using K-anonymization depending on what and how much to share. 
Step 6: L-diversity applied to quasi identifier attributes. 
Step7: Mapped results are fetched from HDFS with the help of HBase table 

Thus proposed approach makes sure that shared data will not disclose the original data and segregation of sensitive data happens before it is a move to HDFS. 

A. SYSTEM MODEL 

Consider N as Health care Data and A as set of Individual attributes where a∈A {In, Qm}. In =Identifier and Qm =Quasi identifier, a common element between multiple attributes like Pincode. α index is an identifier for an individual attribute which is present in the anonymized data set. α index Æ In∈N*, where N* anonymized version of dataset N. The probability of individual attribute being part of the dataset should be less than equal to α and Pr(i∈ N) <= α.

β index Æ (In, S) ∈ N* where S is sensitive data. The probability of an individual attribute’s sensitive information being part of the dataset is defined as Pr (i,s) ∈ N <= β. 

When Pr(I) Pr(I, S) ≠ 0 for any individual there exist a sensitive data. 

Apply K anonymization and L Diversity by masking personal information or the individual attributes I1, I2, I3, .. In. The personal Identifier is the masked and unique random label is assigned to each element is assigned. All individual attributes grouped to K number of groups, G such that  summation of individual elements in K groups = ∑௄ ௜ୀଵ Gi 

For i ≠j and GiGj ≠ 0, which means all individual entries will be part of only one group. 

This helps in sharing the anonymized data without revealing quasi identifier details about individual attributes. 

The masked anonymized and grouped data produces two identifier labels P and Q such that Pi Pj ≠ 0, which means each label, is unique. This indicates that fetching one information will not reveal any other related information. When multiple quasi-identifiers exist in data, to eliminate that each quasi identifier is given separate label so that ∑ଶ ௜ୀଵ ܲ݅=Qi 

VI. CONCLUSION 
Data protection is a major problem in big data and hence necessitates stable methods to the protection of privacy. The Privacy of personally identifiable information in big data health care must be safeguarded so that it does not impede with the privacy of particular patients or healthcare organizations transactions. The proposed approach employs K-anonymization and L-diversity to disguise the personal sensitive information and apache spark to process health care big data in an efficient way. This guarantee that the actual data is not revealed by shared data and isolation of sensitive data occurs before it is even moved to HDFS.